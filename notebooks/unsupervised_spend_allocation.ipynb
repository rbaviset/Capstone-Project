{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58514e8-ac28-4dd8-b8e7-2a4241fae55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/rambavisetty/anaconda_projects/capstone\n",
      "Input file  : /Users/rambavisetty/anaconda_projects/capstone/data_processed/historical_features_with_psl.csv\n",
      "Output file : /Users/rambavisetty/anaconda_projects/capstone/data_processed/historical_features_with_allocation.csv\n",
      "\n",
      "Loaded rows: 30\n",
      "Columns: ['supplier', 'fiscal_year', 'revenue', 'COGS', 'gross_margin_pct', 'cash_flow', 'debt_equity_ratio', 'cost_savings', 'PPV', 'QP', 'QR', 'lead_time_attainment', 'carbon_emission_intensity', 'renewable_energy_usage', 'plastic_recycle', 'human_rights_compliance_score', 'node_parity', 'DDR_gen_support', 'geo_risk', 'tariff_risk', 'chip_shortage_impact', 'supplier_code', 'PSL_cluster', 'PSL_status']\n",
      "\n",
      "Composite scores look OK (no negatives).\n",
      "\n",
      "Allocation % sum by fiscal_year:\n",
      "fiscal_year\n",
      "2015    100.0\n",
      "2016    100.0\n",
      "2017    100.0\n",
      "2018    100.0\n",
      "2019    100.0\n",
      "2020    100.0\n",
      "2021    100.0\n",
      "2022    100.0\n",
      "2023    100.0\n",
      "2024    100.0\n",
      "Name: allocation_percent, dtype: float64\n",
      "\n",
      "Saved historical features with allocation → /Users/rambavisetty/anaconda_projects/capstone/data_processed/historical_features_with_allocation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/7g701x7x7dq4n4zmnhz93_7w0000gn/T/ipykernel_3786/4147515838.py:176: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(to_allocation)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Resolve project paths (works from root or notebooks/)\n",
    "# --------------------------------------------------\n",
    "cwd = os.getcwd()\n",
    "if os.path.basename(cwd).lower() == \"notebooks\":\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "\n",
    "input_path  = os.path.join(PROJECT_ROOT, \"data_processed\", \"historical_features_with_psl.csv\")\n",
    "output_path = os.path.join(PROJECT_ROOT, \"data_processed\", \"historical_features_with_allocation.csv\")\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Input file  :\", input_path)\n",
    "print(\"Output file :\", output_path)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Load data\n",
    "# --------------------------------------------------\n",
    "df = pd.read_csv(input_path)\n",
    "print(\"\\nLoaded rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. PSL score (from PSL_status)\n",
    "# --------------------------------------------------\n",
    "# This mirrors the original logic: Preferred > Developing > Limited\n",
    "psl_score_map = {\n",
    "    \"Preferred\": 1.0,\n",
    "    \"Developing\": 0.6,\n",
    "    \"Limited\": 0.3\n",
    "}\n",
    "\n",
    "if \"PSL_status\" not in df.columns:\n",
    "    raise ValueError(\"Expected column 'PSL_status' in historical_features_with_psl.csv\")\n",
    "\n",
    "df[\"PSL_score\"] = df[\"PSL_status\"].map(psl_score_map)\n",
    "\n",
    "# If any PSL_status values are missing in the map, warn:\n",
    "if df[\"PSL_score\"].isna().any():\n",
    "    missing = df[df[\"PSL_score\"].isna()][\"PSL_status\"].unique()\n",
    "    print(\"\\nWARNING: Unmapped PSL_status values:\", missing)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Define metric groups\n",
    "# --------------------------------------------------\n",
    "# Cost metrics – higher is better (after normalization)\n",
    "score_cols_cost = [\"cost_savings\", \"PPV\"]        # We'll flip PPV so lower PPV => higher score\n",
    "\n",
    "# Quality & delivery – higher better\n",
    "score_cols_qd   = [\"QP\", \"QR\", \"lead_time_attainment\"]\n",
    "\n",
    "# ESG metrics – some \"lower better\", some \"higher better\"\n",
    "# Here we assume:\n",
    "#   - carbon_emission_intensity: lower is better (we will invert)\n",
    "#   - renewable_energy_usage, plastic_recycle, human_rights_compliance_score: higher better\n",
    "score_cols_esg_hi = [\"renewable_energy_usage\", \"plastic_recycle\", \"human_rights_compliance_score\"]\n",
    "col_carbon = \"carbon_emission_intensity\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Safe min–max normalization helper\n",
    "# --------------------------------------------------\n",
    "def safe_minmax(series):\n",
    "    \"\"\"Min-max scale to [0,1]; if constant or NaN, return 0.5.\"\"\"\n",
    "    s = series.astype(float)\n",
    "    if s.isna().all():\n",
    "        return pd.Series(0.5, index=s.index)\n",
    "    vmin, vmax = s.min(), s.max()\n",
    "    if vmax == vmin:\n",
    "        return pd.Series(0.5, index=s.index)\n",
    "    return (s - vmin) / (vmax - vmin)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Build normalized feature scores (0–1)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Cost_savings: higher better → min-max directly\n",
    "if \"cost_savings\" in df.columns:\n",
    "    df[\"score_cost_savings\"] = safe_minmax(df[\"cost_savings\"])\n",
    "else:\n",
    "    df[\"score_cost_savings\"] = 0.5\n",
    "\n",
    "# PPV: typically negative or smaller is better → invert sign then normalize\n",
    "if \"PPV\" in df.columns:\n",
    "    df[\"score_PPV\"] = safe_minmax(-df[\"PPV\"])\n",
    "else:\n",
    "    df[\"score_PPV\"] = 0.5\n",
    "\n",
    "# Quality / Delivery metrics\n",
    "for col in score_cols_qd:\n",
    "    if col in df.columns:\n",
    "        df[f\"score_{col}\"] = safe_minmax(df[col])\n",
    "    else:\n",
    "        df[f\"score_{col}\"] = 0.5\n",
    "\n",
    "# ESG: carbon_emission_intensity (lower better) + others (higher better)\n",
    "if col_carbon in df.columns:\n",
    "    df[\"score_carbon\"] = safe_minmax(-df[col_carbon])  # invert\n",
    "else:\n",
    "    df[\"score_carbon\"] = 0.5\n",
    "\n",
    "for col in score_cols_esg_hi:\n",
    "    if col in df.columns:\n",
    "        df[f\"score_{col}\"] = safe_minmax(df[col])\n",
    "    else:\n",
    "        df[f\"score_{col}\"] = 0.5\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. Combine into composite score (all >= 0)\n",
    "# --------------------------------------------------\n",
    "# You can adjust these weights to match your original tuned logic\n",
    "weight_psl   = 0.5\n",
    "weight_cost  = 0.1\n",
    "weight_qd    = 0.3\n",
    "weight_esg   = 0.1\n",
    "\n",
    "# Cost block: average of cost_savings + PPV scores\n",
    "df[\"score_cost_block\"] = (df[\"score_cost_savings\"] + df[\"score_PPV\"]) / 2.0\n",
    "\n",
    "# QD block\n",
    "df[\"score_qd_block\"] = (\n",
    "    df[\"score_QP\"] + df[\"score_QR\"] + df[\"score_lead_time_attainment\"]\n",
    ") / 3.0\n",
    "\n",
    "# ESG block\n",
    "df[\"score_esg_block\"] = (\n",
    "    df[\"score_carbon\"]\n",
    "    + df[\"score_renewable_energy_usage\"]\n",
    "    + df[\"score_plastic_recycle\"]\n",
    "    + df[\"score_human_rights_compliance_score\"]\n",
    ") / 4.0\n",
    "\n",
    "# Final composite (always non-negative because all components are in [0,1] and weights ≥ 0)\n",
    "df[\"composite_score\"] = (\n",
    "    weight_psl  * df[\"PSL_score\"]\n",
    "    + weight_cost * df[\"score_cost_block\"]\n",
    "    + weight_qd   * df[\"score_qd_block\"]\n",
    "    + weight_esg  * df[\"score_esg_block\"]\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "if (df[\"composite_score\"] < 0).any():\n",
    "    print(\"\\nWARNING: Composite score has negatives. Check logic.\")\n",
    "else:\n",
    "    print(\"\\nComposite scores look OK (no negatives).\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. Convert composite scores → allocation % per fiscal_year\n",
    "# --------------------------------------------------\n",
    "if \"fiscal_year\" not in df.columns:\n",
    "    raise ValueError(\"Expected column 'fiscal_year' in historical_features_with_psl.csv\")\n",
    "\n",
    "def to_allocation(group):\n",
    "    total = group[\"composite_score\"].sum()\n",
    "    if total <= 0:\n",
    "        # Fallback: equal split if something unexpected happens\n",
    "        n = len(group)\n",
    "        return pd.Series([100.0 / n] * n, index=group.index)\n",
    "    raw = group[\"composite_score\"] / total * 100.0\n",
    "    # Round to nearest 5%\n",
    "    rounded = np.round(raw / 5.0) * 5.0\n",
    "    # Fix rounding drift to ensure exact 100%\n",
    "    diff = 100.0 - rounded.sum()\n",
    "    if abs(diff) >= 2.5:\n",
    "        # Adjust the largest supplier by the residual\n",
    "        idx_max = rounded.idxmax()\n",
    "        rounded.loc[idx_max] += diff\n",
    "    return rounded\n",
    "\n",
    "df[\"allocation_percent\"] = (\n",
    "    df.groupby(\"fiscal_year\", group_keys=False)\n",
    "      .apply(to_allocation)\n",
    ")\n",
    "\n",
    "# Clamp to [0,100] for safety\n",
    "df[\"allocation_percent\"] = df[\"allocation_percent\"].clip(lower=0, upper=100)\n",
    "\n",
    "# Validate per-year sum\n",
    "check = df.groupby(\"fiscal_year\")[\"allocation_percent\"].sum()\n",
    "print(\"\\nAllocation % sum by fiscal_year:\")\n",
    "print(check)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 9. Save output\n",
    "# --------------------------------------------------\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"\\nSaved historical features with allocation →\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cfc36-3027-492f-8a99-8414c40fbd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67537c4-bdf5-42e8-9e6b-0c29553d7e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d085ff-e7d8-4d72-b40c-66e4d0c7aafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b358dcc-93f1-47be-a575-367b64df4756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
